"""
Enhanced LLM Service for Vimarsh - Production-Ready Spiritual Guidance

This module provides a comprehensive LLM service interface that connects to 
real Gemini 2.5 Flash API with advanced safety controls, spiritual context awareness,
token tracking, and content validation specifically designed for spiritual guidance.
"""

import logging
import os
import time
import re
from typing import Dict, Any, Optional, List
from dataclasses import dataclass, field
from enum import Enum
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold

# Import unified configuration
try:
    from backend.config.unified_config import get_unified_config
    CONFIG_AVAILABLE = True
except ImportError:
    CONFIG_AVAILABLE = False

# Import database service for real citations
try:
    from .database_service import db_service
    DATABASE_AVAILABLE = True
except ImportError:
    DATABASE_AVAILABLE = False

# Import personality and prompt template services
try:
    from .personality_service import personality_service, PersonalityProfile
    from .prompt_template_service import prompt_template_service, TemplateRenderContext
    PERSONALITY_SERVICES_AVAILABLE = True
except ImportError:
    PERSONALITY_SERVICES_AVAILABLE = False

logger = logging.getLogger(__name__)


class SafetyLevel(Enum):
    """Safety levels for spiritual content generation"""
    STRICT = "strict"          # Maximum safety for public deployment
    MODERATE = "moderate"      # Balanced safety for development
    MINIMAL = "minimal"        # Minimal safety for testing


class SpiritualContext(Enum):
    """Types of spiritual contexts for appropriate responses"""
    GUIDANCE = "guidance"              # Personal spiritual guidance
    TEACHING = "teaching"              # Educational content about scriptures
    PHILOSOPHY = "philosophy"          # Philosophical discussions
    DEVOTIONAL = "devotional"          # Devotional practices and prayers
    MEDITATION = "meditation"          # Meditation and contemplative practices
    PERSONAL_GROWTH = "personal_growth"  # Personal development and growth
    GENERAL = "general"               # General spiritual inquiries


@dataclass
class SpiritualSafetyConfig:
    """Configuration for spiritual content safety measures"""
    safety_level: SafetyLevel
    allowed_contexts: List[SpiritualContext]
    require_citations: bool = True
    block_personal_predictions: bool = True
    block_medical_advice: bool = True
    require_reverent_tone: bool = True
    max_response_length: int = 1000
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for logging/debugging"""
        return {
            "safety_level": self.safety_level.value,
            "allowed_contexts": [ctx.value for ctx in self.allowed_contexts],
            "require_citations": self.require_citations,
            "block_personal_predictions": self.block_personal_predictions,
            "block_medical_advice": self.block_medical_advice,
            "require_reverent_tone": self.require_reverent_tone,
            "max_response_length": self.max_response_length
        }


@dataclass
class TokenUsage:
    """Token usage information for cost tracking"""
    input_tokens: int = 0
    output_tokens: int = 0
    total_tokens: int = 0
    estimated_cost: float = 0.0
    model_name: str = "gemini-2.5-flash"


@dataclass
class SpiritualResponse:
    """Enhanced spiritual response structure with safety and validation"""
    content: str
    citations: List[str] = field(default_factory=list)
    confidence: float = 0.8
    language: str = "English"
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    # Enhanced fields from old system
    spiritual_context: Optional[SpiritualContext] = None
    safety_ratings: Dict[str, Any] = field(default_factory=dict)
    safety_passed: bool = True
    safety_score: float = 1.0
    finish_reason: str = "STOP"
    token_usage: Optional[TokenUsage] = None
    response_time: float = 0.0
    warnings: List[str] = field(default_factory=list)
    
    # Validation flags
    content_validated: bool = True
    citations_verified: bool = False
    reverent_tone_checked: bool = False


class EnhancedLLMService:
    """
    Production-ready LLM service for spiritual guidance with comprehensive safety controls,
    context awareness, token tracking, and content validation.
    """
    
    def __init__(self, api_key: Optional[str] = None, safety_config: Optional[SpiritualSafetyConfig] = None):
        """
        Initialize enhanced LLM service with safety configuration.
        
        Args:
            api_key: Google AI API key (if None, reads from unified configuration)
            safety_config: Spiritual safety configuration
        """
        # Load configuration from unified config system
        config_loaded = False
        try:
            from backend.config.unified_config import get_unified_config
            config = get_unified_config()
            self.api_key = api_key or config.get_value("LLM", "GEMINI_API_KEY", fallback="")
            self.model_name = config.get_value("LLM", "MODEL", fallback="gemini-2.5-flash")
            self.temperature = float(config.get_value("LLM", "TEMPERATURE", fallback=0.7))
            self.max_tokens = int(config.get_value("LLM", "MAX_TOKENS", fallback=150))
            self.safety_settings_level = config.get_value("LLM", "SAFETY_SETTINGS", fallback="BLOCK_MEDIUM_AND_ABOVE")
            logger.info("ðŸ”§ LLM service configured using unified configuration")
            config_loaded = True
        except Exception as e:
            logger.warning(f"Failed to load unified config for LLM service: {e}")
            # Fall back to manual configuration
        
        if not config_loaded:
            # Fallback to environment variables
            self.api_key = api_key or os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_AI_API_KEY")
            self.model_name = os.getenv("LLM_MODEL", "gemini-2.5-flash")
            self.temperature = float(os.getenv("LLM_TEMPERATURE", "0.7"))
            self.max_tokens = int(os.getenv("MAX_TOKENS", "150"))  # Reduced for concise responses
            self.safety_settings_level = os.getenv("SAFETY_SETTINGS", "BLOCK_MEDIUM_AND_ABOVE")
            logger.info("ðŸ”§ LLM service configured using environment variables (fallback)")
        
        self.is_configured = bool(self.api_key and self.api_key != "dev-mode-placeholder")
        
        # Set up safety configuration
        self.safety_config = safety_config or self._default_safety_config()
        
        if self.is_configured:
            # Configure Gemini 2.5 Flash with safety settings
            genai.configure(api_key=self.api_key)
            self.model = self._initialize_model()
            logger.info(f"âœ… Enhanced Gemini 2.5 Flash API configured with safety level: {self.safety_config.safety_level.value}")
        else:
            logger.warning("GEMINI_API_KEY not found - using enhanced fallback responses")
            self.model = None
    
    def _default_safety_config(self) -> SpiritualSafetyConfig:
        """Create default safety configuration for spiritual guidance"""
        return SpiritualSafetyConfig(
            safety_level=SafetyLevel.MODERATE,
            allowed_contexts=[
                SpiritualContext.GUIDANCE,
                SpiritualContext.TEACHING,
                SpiritualContext.PHILOSOPHY,
                SpiritualContext.DEVOTIONAL,
                SpiritualContext.MEDITATION,
                SpiritualContext.PERSONAL_GROWTH,
                SpiritualContext.GENERAL
            ],
            require_citations=True,
            block_personal_predictions=True,
            block_medical_advice=True,
            require_reverent_tone=True,
            max_response_length=500  # Updated to Krishna's optimal limit (learned from simple service)
        )
    
    def _initialize_model(self) -> genai.GenerativeModel:
        """Initialize Gemini model with safety settings"""
        safety_settings = self._get_safety_settings()
        
        generation_config = {
            "temperature": self.temperature,
            "top_p": 0.8,
            "top_k": 40,
            "max_output_tokens": min(self.max_tokens, self.safety_config.max_response_length),
        }
        
        return genai.GenerativeModel(
            model_name=self.model_name,
            generation_config=generation_config,
            safety_settings=safety_settings
        )
    
    def _get_safety_settings(self) -> List[Dict[str, Any]]:
        """Configure safety settings based on safety level"""
        if self.safety_config.safety_level == SafetyLevel.STRICT:
            threshold = HarmBlockThreshold.BLOCK_LOW_AND_ABOVE
        elif self.safety_config.safety_level == SafetyLevel.MODERATE:
            threshold = HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE
        else:  # MINIMAL
            threshold = HarmBlockThreshold.BLOCK_ONLY_HIGH
        
        return [
            {"category": HarmCategory.HARM_CATEGORY_HARASSMENT, "threshold": threshold},
            {"category": HarmCategory.HARM_CATEGORY_HATE_SPEECH, "threshold": threshold},
            {"category": HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, "threshold": threshold},
            {"category": HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, "threshold": threshold},
        ]
    
    async def _create_personality_system_prompt(
        self, 
        personality_id: str, 
        context: SpiritualContext,
        conversation_context: List[Dict[str, str]] = None
    ) -> str:
        """Create personality-specific system prompt using template service"""
        try:
            if PERSONALITY_SERVICES_AVAILABLE:
                # Get personality profile
                personality = await personality_service.get_personality(personality_id)
                if not personality:
                    logger.warning(f"Personality {personality_id} not found, falling back to Krishna")
                    return self._create_fallback_spiritual_prompt(context, personality_id)
                
                # Create template render context
                from .prompt_template_service import TemplateRenderContext
                template_context = TemplateRenderContext(
                    personality_id=personality_id,
                    domain=personality.domain.value,
                    query="",  # Will be filled in later
                    conversation_history=conversation_context or [],
                    language="English"
                )
                
                # Try to render personality-specific template
                try:
                    return await prompt_template_service.render_personality_prompt(
                        personality_id=personality_id,
                        query="${query}",  # Placeholder for later substitution
                        context_chunks=[],  # Will be filled with actual context
                        language="English",
                        conversation_history=conversation_context
                    )
                except Exception as e:
                    logger.warning(f"Failed to render personality template for {personality_id}: {e}")
                    # Fall back to domain-specific prompt
                    return self._create_domain_specific_prompt(personality, context)
            else:
                logger.warning("Personality services not available, using fallback")
                return self._create_fallback_spiritual_prompt(context, personality_id)
                
        except Exception as e:
            logger.error(f"Error creating personality prompt for {personality_id}: {e}")
            return self._create_fallback_spiritual_prompt(context, personality_id)
    
    def _create_domain_specific_prompt(self, personality, context: SpiritualContext) -> str:
        """Create domain-specific prompt based on personality"""
        if personality.domain.value == "spiritual":
            return self._create_spiritual_personality_prompt(personality, context)
        elif personality.domain.value == "scientific":
            return self._create_scientific_personality_prompt(personality, context)
        elif personality.domain.value == "historical":
            return self._create_historical_personality_prompt(personality, context)
        elif personality.domain.value == "philosophical":
            return self._create_philosophical_personality_prompt(personality, context)
        else:
            return self._create_fallback_spiritual_prompt(context, personality_id)
    
    def _create_spiritual_personality_prompt(self, personality, context: SpiritualContext) -> str:
        """Create spiritual personality prompt - Updated with proven working approach"""
        
        # Use proven Krishna approach if this is Krishna personality
        if personality.id == "krishna" or "krishna" in personality.name.lower():
            base_prompt = """You are Lord Krishna from the Bhagavad Gita. Answer this spiritual question briefly and authentically.

RESPONSE REQUIREMENTS:
- Maximum 400-500 characters 
- Include Sanskrit verse or reference when relevant
- Use authentic Krishna voice with divine authority
- Be warm and compassionate
- Keep response focused and concise
- End with blessing like "ðŸ™"

USER QUERY: ${query}

Response:"""
            
            context_specific = {
                SpiritualContext.GUIDANCE: "\nCONTEXT: Provide personal spiritual guidance from the Bhagavad Gita's wisdom.",
                SpiritualContext.TEACHING: "\nCONTEXT: Teach from the Bhagavad Gita with Sanskrit references.",
                SpiritualContext.PHILOSOPHY: "\nCONTEXT: Share the philosophical truths of dharma and divine consciousness.",
                SpiritualContext.DEVOTIONAL: "\nCONTEXT: Guide in bhakti and devotional surrender to the Divine.",
                SpiritualContext.MEDITATION: "\nCONTEXT: Teach dhyana and Raja Yoga from the Gita's teachings.",
                SpiritualContext.PERSONAL_GROWTH: "\nCONTEXT: Support spiritual growth through dharmic action and devotion.",
                SpiritualContext.GENERAL: "\nCONTEXT: Answer with Krishna's divine wisdom and compassion."
            }
            
            return base_prompt + context_specific.get(context, context_specific[SpiritualContext.GENERAL])
        
        # For other spiritual personalities, use the original detailed approach
        base_prompt = f"""You are {personality.name}, {personality.description}. You embody the wisdom and compassion of the spiritual tradition you represent.

PERSONALITY CHARACTERISTICS:
- Tone: {personality.tone_characteristics.get('formality', 'reverent')}, {personality.tone_characteristics.get('warmth', 'compassionate')}
- Cultural Context: {personality.cultural_context}
- Expertise Areas: {', '.join(personality.expertise_areas)}
- Language Style: {personality.language_style}

RESPONSE REQUIREMENTS:
- Keep responses concise (maximum 100-150 words)
- Include specific citations from your associated texts when relevant
- Begin with appropriate greeting for your tradition
- Provide wisdom appropriate to your spiritual background
- End with a blessing appropriate to your tradition
- Use proper markdown formatting for emphasis

GREETING PATTERNS: {', '.join(personality.greeting_patterns) if personality.greeting_patterns else 'Use appropriate spiritual greeting'}
FAREWELL PATTERNS: {', '.join(personality.farewell_patterns) if personality.farewell_patterns else 'Use appropriate spiritual blessing'}

SAFETY GUIDELINES:
- No medical, legal, or professional advice
- No personal predictions about the future
- Maintain authenticity to your spiritual tradition
- Redirect inappropriate questions to spiritual matters

CONTEXT FROM YOUR TEACHINGS:
${{context_chunks}}

CONVERSATION HISTORY:
${{conversation_history}}

USER QUERY: ${{query}}

Response:"""

        context_specific = {
            SpiritualContext.GUIDANCE: "\nCONTEXT: Provide personal spiritual guidance for life challenges, helping the seeker find inner peace and right action through your tradition's principles.",
            SpiritualContext.TEACHING: "\nCONTEXT: Share educational content about your spiritual tradition, philosophy, and practices with clarity and authenticity.",
            SpiritualContext.PHILOSOPHY: "\nCONTEXT: Engage in philosophical discussions about the nature of reality, consciousness, and spiritual truth as revealed in your teachings.",
            SpiritualContext.DEVOTIONAL: "\nCONTEXT: Guide devotional practices and cultivating love and surrender to the Divine according to your tradition.",
            SpiritualContext.MEDITATION: "\nCONTEXT: Teach meditation techniques and methods for achieving inner stillness according to your spiritual path.",
            SpiritualContext.PERSONAL_GROWTH: "\nCONTEXT: Support spiritual development and the cultivation of divine qualities according to your teachings.",
            SpiritualContext.GENERAL: "\nCONTEXT: Address general spiritual inquiries with wisdom appropriate to your tradition."
        }
        
        return base_prompt + context_specific.get(context, context_specific[SpiritualContext.GENERAL])
    
    def _create_scientific_personality_prompt(self, personality, context: SpiritualContext) -> str:
        """Create scientific personality prompt"""
        return f"""You are {personality.name}, {personality.description}.

PERSONALITY CHARACTERISTICS:
- Approach: {personality.tone_characteristics.get('formality', 'academic')}, {personality.tone_characteristics.get('warmth', 'curious')}
- Time Period: {personality.time_period if hasattr(personality, 'time_period') else 'Modern era'}
- Expertise Areas: {', '.join(personality.expertise_areas)}
- Communication Style: {personality.language_style}

RESPONSE REQUIREMENTS:
- Provide scientifically accurate responses based on your documented work
- Use clear explanations that make complex concepts accessible
- Reference your published papers or documented statements when relevant
- Maintain intellectual curiosity and openness to inquiry
- Keep responses focused and informative (100-200 words)

GREETING STYLE: {personality.response_patterns.get('greeting_style', 'My friend')}

CONTEXT FROM YOUR WORK:
${{context_chunks}}

CONVERSATION HISTORY:
${{conversation_history}}

SCIENTIFIC INQUIRY: ${{query}}

If the question falls outside your documented expertise, acknowledge the limits of your knowledge while suggesting related areas you can address.

Response:"""
    
    def _create_historical_personality_prompt(self, personality, context: SpiritualContext) -> str:
        """Create historical personality prompt"""
        return f"""You are {personality.name}, {personality.description}, speaking from the perspective of {getattr(personality, 'time_period', 'your historical era')}.

PERSONALITY CHARACTERISTICS:
- Historical Context: {personality.cultural_context}
- Leadership Style: {personality.tone_characteristics.get('authority', 'principled')}
- Communication: {personality.language_style}
- Expertise Areas: {', '.join(personality.expertise_areas)}

RESPONSE REQUIREMENTS:
- Speak from your historical perspective and lived experiences
- Reference your documented speeches, writings, or recorded statements
- Provide wisdom gained from your life experiences
- Use language appropriate to your era while remaining accessible
- Keep responses thoughtful and substantial (100-200 words)

GREETING STYLE: {personality.response_patterns.get('greeting_style', 'My fellow citizen')}

HISTORICAL CONTEXT AND DOCUMENTS:
${{context_chunks}}

CONVERSATION HISTORY:
${{conversation_history}}

QUESTION: ${{query}}

If asked about events after your time, acknowledge these limitations while offering relevant insights from your historical perspective.

Response:"""
    
    def _create_philosophical_personality_prompt(self, personality, context: SpiritualContext) -> str:
        """Create philosophical personality prompt"""
        return f"""You are {personality.name}, {personality.description}.

PERSONALITY CHARACTERISTICS:
- Philosophical Approach: {personality.tone_characteristics.get('teaching_style', 'contemplative')}
- Cultural Background: {personality.cultural_context}
- Areas of Wisdom: {', '.join(personality.expertise_areas)}
- Communication Style: {personality.language_style}

RESPONSE REQUIREMENTS:
- Provide philosophical insights based on your documented teachings
- Use contemplative and thoughtful language
- Reference your philosophical works when relevant
- Encourage deeper reflection and self-examination
- Keep responses profound yet accessible (100-200 words)

GREETING STYLE: {personality.response_patterns.get('greeting_style', 'Fellow seeker of wisdom')}

PHILOSOPHICAL CONTEXT:
${{context_chunks}}

CONVERSATION HISTORY:
${{conversation_history}}

PHILOSOPHICAL INQUIRY: ${{query}}

Response:"""
    
    def _create_fallback_spiritual_prompt(self, context: SpiritualContext, personality_id: str = "krishna") -> str:
        """Create fallback prompt based on personality domain"""
        # Default to Krishna if personality_id not provided or not found
        if personality_id == "krishna" or not personality_id:
            return self._create_krishna_fallback_prompt(context)
        elif personality_id == "einstein":
            return self._create_einstein_fallback_prompt(context)
        elif personality_id == "lincoln":
            return self._create_lincoln_fallback_prompt(context)
        elif personality_id == "marcus_aurelius":
            return self._create_marcus_aurelius_fallback_prompt(context)
        else:
            # Unknown personality, default to Krishna
            return self._create_krishna_fallback_prompt(context)
    
    def _create_krishna_fallback_prompt(self, context: SpiritualContext) -> str:
        """Krishna-specific fallback prompt - Updated with proven working approach"""
        base_prompt = """You are Lord Krishna from the Bhagavad Gita. Answer this spiritual question briefly and authentically.

REQUIREMENTS:
- Start with "Beloved devotee," or "My beloved devotee,"
- Keep response to 400-500 characters maximum
- Include at least one relevant Sanskrit verse with translation (like "à¤•à¤°à¥à¤®à¤£à¥à¤¯à¥‡à¤µà¤¾à¤§à¤¿à¤•à¤¾à¤°à¤¸à¥à¤¤à¥‡...")
- Focus on practical spiritual guidance from the Gita
- Use authentic Krishna voice with divine authority
- Provide specific verse citations when possible

CONTEXT FROM SACRED TEXTS:
${context_chunks}

CONVERSATION HISTORY:
${conversation_history}

USER QUERY: ${query}

Your response:"""

        context_specific = {
            SpiritualContext.GUIDANCE: "\nCONTEXT: Provide personal spiritual guidance for life challenges, helping the seeker find inner peace and right action through dharmic principles.",
            SpiritualContext.TEACHING: "\nCONTEXT: Share educational content about Hindu scriptures, philosophy, and spiritual practices with clarity and authenticity.",
            SpiritualContext.PHILOSOPHY: "\nCONTEXT: Engage in philosophical discussions about the nature of reality, consciousness, and spiritual truth as revealed in Vedantic teachings.",
            SpiritualContext.DEVOTIONAL: "\nCONTEXT: Guide devotional practices, bhakti yoga, and cultivating love and surrender to the Divine.",
            SpiritualContext.MEDITATION: "\nCONTEXT: Teach meditation techniques, mindfulness practices, and methods for achieving inner stillness and self-realization.",
            SpiritualContext.PERSONAL_GROWTH: "\nCONTEXT: Support spiritual development, character building, and the cultivation of divine qualities like compassion, patience, and wisdom.",
            SpiritualContext.GENERAL: "\nCONTEXT: Address general spiritual inquiries with wisdom appropriate to the seeker's level of understanding."
        }
        
        return base_prompt + context_specific.get(context, context_specific[SpiritualContext.GENERAL])
    
    def _create_einstein_fallback_prompt(self, context: SpiritualContext) -> str:
        """Einstein-specific fallback prompt"""
        return """You are Albert Einstein, the renowned theoretical physicist. You speak with scientific precision, intellectual curiosity, and philosophical depth.

RESPONSE REQUIREMENTS:
- Keep responses concise (maximum 100-150 words)
- Reference your scientific work when relevant
- Begin with "My friend," or "Greetings,"
- Provide scientifically accurate information
- Show intellectual curiosity and wonder
- Use thought experiments when helpful

CONTEXT FROM YOUR WORK:
${context_chunks}

CONVERSATION HISTORY:
${conversation_history}

SCIENTIFIC INQUIRY: ${query}

Response:"""
    
    def _create_lincoln_fallback_prompt(self, context: SpiritualContext) -> str:
        """Lincoln-specific fallback prompt"""
        return """You are Abraham Lincoln, 16th President of the United States. You speak with dignity, compassion, and moral authority.

RESPONSE REQUIREMENTS:
- Keep responses thoughtful (maximum 100-150 words)
- Reference your speeches or experiences when relevant
- Begin with "My fellow citizen," or "Friend,"
- Provide wisdom from your leadership experience
- Show commitment to democracy and equality
- Use storytelling when appropriate

CONTEXT FROM YOUR SPEECHES:
${context_chunks}

CONVERSATION HISTORY:
${conversation_history}

QUESTION: ${query}

Response:"""
    
    def _create_marcus_aurelius_fallback_prompt(self, context: SpiritualContext) -> str:
        """Marcus Aurelius-specific fallback prompt"""
        return """You are Marcus Aurelius, Roman Emperor and Stoic philosopher. You speak with philosophical depth and Stoic wisdom.

RESPONSE REQUIREMENTS:
- Keep responses contemplative (maximum 100-150 words)
- Reference your Meditations when relevant
- Begin with "Fellow seeker," or "Friend,"
- Provide Stoic philosophical insights
- Show understanding of virtue and duty
- Maintain philosophical composure

CONTEXT FROM YOUR MEDITATIONS:
${context_chunks}

CONVERSATION HISTORY:
${conversation_history}

PHILOSOPHICAL INQUIRY: ${query}

Response:"""
    
    def _validate_spiritual_content(self, content: str, context: SpiritualContext, personality_id: str = "krishna") -> List[str]:
        """Validate content for appropriateness based on personality"""
        warnings = []
        
        # Check for appropriate greeting based on personality
        personality_greetings = {
            "krishna": ["beloved devotee", "dear soul", "dear seeker", "beloved child"],
            "einstein": ["my friend", "greetings", "hello", "welcome"],
            "lincoln": ["my fellow citizen", "friend", "good day", "greetings"],
            "marcus_aurelius": ["fellow seeker", "friend", "greetings", "hail"]
        }
        
        appropriate_greetings = personality_greetings.get(personality_id, personality_greetings["krishna"])
        if not any(greeting in content.lower() for greeting in appropriate_greetings):
            warnings.append(f"Response should begin with appropriate greeting for {personality_id}")
        
        # Check for inappropriate emoji usage (mainly for Krishna)
        if personality_id == "krishna" and content.startswith("ðŸ™"):
            warnings.append("Response should not begin with folded hands emoji")
        
        # Check for harmful content patterns
        harmful_patterns = [
            r"medical advice",
            r"legal advice", 
            r"financial advice",
            r"predict.*future",
            r"miracle.*cure",
            r"guaranteed.*result"
        ]
        
        for pattern in harmful_patterns:
            if re.search(pattern, content.lower()):
                warnings.append(f"Content may contain inappropriate advice: {pattern}")
        
        # Check length
        if len(content) > self.safety_config.max_response_length:
            warnings.append(f"Response too long: {len(content)} > {self.safety_config.max_response_length}")
        
        return warnings
    
    def _extract_citations_advanced(self, content: str) -> List[str]:
        """Advanced citation extraction with multiple pattern matching"""
        citations = []
        
        # Citation patterns for Hindu scriptures
        citation_patterns = [
            r'Bhagavad Gita \d+\.\d+',
            r'BG \d+\.\d+',
            r'Srimad Bhagavatam \d+\.\d+\.\d+',
            r'SB \d+\.\d+\.\d+',
            r'Mahabharata \d+\.\d+',
            r'Ramayana \d+\.\d+',
            r'Upanishads?',
            r'Vedas?',
            r'Puranas?'
        ]
        
        for pattern in citation_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            citations.extend(matches)
        
        # Remove duplicates and return
        return list(set(citations))
    
    def _estimate_token_cost(self, input_tokens: int, output_tokens: int) -> float:
        """Estimate API cost based on token usage"""
        # Gemini 2.5 Flash pricing (approximate)
        input_cost_per_1k = 0.0005  # $0.0005 per 1K input tokens
        output_cost_per_1k = 0.0015  # $0.0015 per 1K output tokens
        
        input_cost = (input_tokens / 1000) * input_cost_per_1k
        output_cost = (output_tokens / 1000) * output_cost_per_1k
        
        return input_cost + output_cost
    
    def _estimate_tokens(self, text: str) -> int:
        """Rough token estimation (1 token â‰ˆ 4 characters)"""
        return len(text) // 4
    
    async def generate_multi_personality_response(
        self, 
        query: str, 
        context: str = "general", 
        conversation_context: List[Dict[str, str]] = None,
        personality_id: str = None
    ) -> SpiritualResponse:
        """
        Generate response from any personality with comprehensive safety analysis and validation.
        
        Args:
            query: The question or inquiry
            context: Context (guidance, teaching, philosophy, etc.)
            conversation_context: Previous messages for follow-up questions
            personality_id: ID of the personality to use for response generation (required)
            
        Returns:
            SpiritualResponse with content, safety analysis, and metadata
        """
        start_time = time.time()
        
        # Validate personality_id is provided
        if not personality_id:
            logger.error("personality_id is required for generate_personality_response")
            return await self._get_error_response(query, "Personality ID is required", time.time() - start_time, "unknown")
        
        # Convert context string to enum
        spiritual_context = self._parse_context(context)
        
        if not self.is_configured:
            return self._get_fallback_response(query, context, personality_id)
        
        try:
            # Create personality-specific system prompt
            spiritual_prompt = await self._create_personality_system_prompt(personality_id, spiritual_context, conversation_context)
            
            # Build full prompt with conversation context
            full_prompt = self._build_contextual_prompt(spiritual_prompt, query, conversation_context)
            
            # Generate response using Gemini 2.5 Flash
            response = self.model.generate_content(full_prompt)
            response_time = time.time() - start_time
            
            # Parse response with comprehensive safety analysis
            return await self._parse_gemini_response_advanced(response, query, spiritual_context, response_time, full_prompt, personality_id)
            
        except Exception as e:
            logger.error(f"Gemini 2.5 Flash API error for personality {personality_id}: {e}")
            response_time = time.time() - start_time
            return await self._get_error_response(query, str(e), response_time, personality_id)
    
    # Maintain backward compatibility
    async def get_spiritual_guidance(
        self, 
        query: str, 
        context: str = "general", 
        conversation_context: List[Dict[str, str]] = None,
        personality_id: str = None
    ) -> SpiritualResponse:
        """
        Legacy method for backward compatibility. Use generate_personality_response instead.
        Note: personality_id is now required for multi-personality support.
        """
        # Require personality_id to be explicitly specified
        if not personality_id:
            logger.error("get_spiritual_guidance called without personality_id. This is required for multi-personality support.")
            raise ValueError("personality_id is required for multi-personality support. Please specify which personality to use.")
        
        return await self.generate_multi_personality_response(query, context, conversation_context, personality_id)
    
    def _parse_context(self, context: str) -> SpiritualContext:
        """Parse context string to SpiritualContext enum"""
        context_mapping = {
            "guidance": SpiritualContext.GUIDANCE,
            "teaching": SpiritualContext.TEACHING,
            "philosophy": SpiritualContext.PHILOSOPHY,
            "devotional": SpiritualContext.DEVOTIONAL,
            "meditation": SpiritualContext.MEDITATION,
            "personal_growth": SpiritualContext.PERSONAL_GROWTH,
            "general": SpiritualContext.GENERAL
        }
        return context_mapping.get(context.lower(), SpiritualContext.GENERAL)
    
    def _build_contextual_prompt(self, spiritual_prompt: str, query: str, conversation_context: List[Dict[str, str]] = None) -> str:
        """Build a contextual prompt that includes conversation history for follow-up questions"""
        
        # Start with the base spiritual prompt
        full_prompt = spiritual_prompt
        
        # Add conversation context if provided
        if conversation_context and len(conversation_context) > 0:
            full_prompt += "\n\nCONVERSATION CONTEXT (for follow-up questions):"
            
            # Only include the last few messages to avoid token limits
            recent_context = conversation_context[-4:] if len(conversation_context) > 4 else conversation_context
            
            for i, message in enumerate(recent_context):
                role = message.get('role', 'user')
                content = message.get('content', '')
                
                if role == 'user':
                    full_prompt += f"\nUser's Previous Question: {content}"
                elif role == 'assistant':
                    full_prompt += f"\nPrevious Response: {content}"
        
        # Add the current question
        full_prompt += f"\n\nCurrent Question: {query}"
        
        # Add enhanced instructions for handling follow-ups
        if conversation_context and len(conversation_context) > 0:
            # Check if this looks like a follow-up question
            follow_up_patterns = ['give more details', 'explain further', 'tell me more', 'can you elaborate', 'expand on', 'more about', 'deeper', 'further explanation']
            is_follow_up = any(pattern in query.lower() for pattern in follow_up_patterns)
            
            if is_follow_up:
                full_prompt += "\n\nIMPORTANT: This is a follow-up question asking for more details. You should:"
                full_prompt += "\n- Begin with a phrase like 'Building on what I shared...' or 'To elaborate further, dear soul...'"
                full_prompt += "\n- Expand on the SAME concept from your previous response"
                full_prompt += "\n- Reference the previous teaching and add deeper insights"
                full_prompt += "\n- Include additional scriptural citations that complement the previous one"
                full_prompt += "\n- Make it feel like a natural continuation of the conversation"
                full_prompt += "\n- Still maintain the concise format but with more depth"
            else:
                full_prompt += "\n\nNOTE: Use conversation context to inform your response, but this appears to be a new question rather than a follow-up."
        
        full_prompt += "\n\nResponse:"
        
        return full_prompt
    
    async def _parse_gemini_response_advanced(self, response, query: str, context: SpiritualContext, 
                                       response_time: float, full_prompt: str, personality_id: str) -> SpiritualResponse:
        """
        Advanced parsing of Gemini 2.5 Flash response with comprehensive safety and metadata analysis.
        
        This method extracts all available metadata from the Gemini response including:
        - Safety ratings and content blocking detection
        - Token usage and cost calculation
        - Finish reason analysis
        - Content validation and warning generation
        """
        try:
            # Extract content with safety blocking detection
            content = ""
            content_blocked = False
            finish_reason = "STOP"
            
            try:
                if hasattr(response, 'text'):
                    if response.text is None:
                        content_blocked = True
                        # Get personality-specific blocked content message
                        blocked_messages = {
                            "krishna": "Dear soul, I apologize, but I cannot provide an appropriate response to that query. Please ask something more suitable for spiritual guidance.",
                            "einstein": "My friend, I cannot provide an appropriate response to that inquiry. Please ask something more suitable for scientific discussion.",
                            "lincoln": "My fellow citizen, I cannot provide an appropriate response to that question. Please ask something more suitable for thoughtful discourse.",
                            "marcus_aurelius": "Fellow seeker, I cannot provide an appropriate response to that inquiry. Please ask something more suitable for philosophical reflection."
                        }
                        content = blocked_messages.get(personality_id, blocked_messages["krishna"])
                    else:
                        content = response.text if response.text else ""
                        if not isinstance(content, str):
                            content = str(content) if content is not None else ""
                else:
                    # Get personality-specific difficulty message
                    difficulty_messages = {
                        "krishna": "Dear soul, I apologize, but I'm having difficulty accessing the spiritual wisdom right now.",
                        "einstein": "My friend, I'm having some difficulty accessing my knowledge at the moment.",
                        "lincoln": "My fellow citizen, I'm experiencing some difficulty accessing my thoughts right now.",
                        "marcus_aurelius": "Fellow seeker, I'm having some difficulty accessing philosophical wisdom at this moment."
                    }
                    content = difficulty_messages.get(personality_id, difficulty_messages["krishna"])
            except Exception:
                # Get personality-specific exception message
                exception_messages = {
                    "krishna": "Dear soul, I apologize, but I'm having difficulty accessing the spiritual wisdom right now.",
                    "einstein": "My friend, I'm experiencing technical difficulties accessing my knowledge.",
                    "lincoln": "My fellow citizen, I'm having some technical difficulties at the moment.",
                    "marcus_aurelius": "Fellow seeker, I'm encountering some technical obstacles right now."
                }
                content = exception_messages.get(personality_id, exception_messages["krishna"])
            
            # Extract safety ratings with comprehensive analysis
            safety_ratings = self._extract_safety_ratings(response)
            
            # Extract finish reason
            finish_reason = self._extract_finish_reason(response)
            
            # Extract and calculate token usage
            token_usage = self._extract_token_usage(response, full_prompt, content)
            
            # Advanced content validation
            content_warnings = self._validate_spiritual_content(content, spiritual_context, personality_id)
            
            # Advanced citation extraction
            citations = self._extract_citations_advanced(content)
            
            # Calculate overall safety status
            safety_passed = not content_blocked and finish_reason in ["STOP", "MAX_TOKENS"] and len(content_warnings) == 0
            safety_score = self._calculate_safety_score(safety_ratings, content_warnings, content_blocked)
            
            # Validate content quality
            content_validated = self._validate_content_quality(content, context)
            citations_verified = self._verify_citations(citations) if citations else False
            reverent_tone_checked = self._check_reverent_tone(content)
            
            return SpiritualResponse(
                content=content,
                citations=citations,
                confidence=0.9 if safety_passed else 0.7,
                language="English",
                metadata={
                    "model": "gemini-2.5-flash",
                    "response_time": response_time,
                    "query_length": len(query),
                    "response_length": len(content),
                    "finish_reason": finish_reason,
                    "content_blocked": content_blocked,
                    "total_tokens": token_usage.total_tokens,
                    "estimated_cost": token_usage.estimated_cost
                },
                spiritual_context=context,
                safety_ratings=safety_ratings,
                safety_passed=safety_passed,
                safety_score=safety_score,
                finish_reason=finish_reason,
                token_usage=token_usage,
                response_time=response_time,
                warnings=content_warnings,
                content_validated=content_validated,
                citations_verified=citations_verified,
                reverent_tone_checked=reverent_tone_checked
            )
            
        except Exception as e:
            logger.error(f"Error parsing Gemini response: {e}")
            return await self._get_error_response(query, str(e), response_time, personality_id)
    
    def _extract_safety_ratings(self, response) -> Dict[str, Any]:
        """Extract comprehensive safety ratings from Gemini response"""
        safety_ratings = {}
        
        try:
            # Extract prompt feedback safety ratings
            if hasattr(response, 'prompt_feedback') and response.prompt_feedback:
                safety_ratings['prompt_feedback'] = {
                    'block_reason': getattr(response.prompt_feedback, 'block_reason', None),
                    'safety_ratings': []
                }
                
                try:
                    prompt_ratings = getattr(response.prompt_feedback, 'safety_ratings', [])
                    if hasattr(prompt_ratings, '__iter__'):
                        safety_ratings['prompt_feedback']['safety_ratings'] = [
                            {
                                'category': getattr(rating, 'category', {}).get('name', 'UNKNOWN') if hasattr(getattr(rating, 'category', {}), 'name') else 'UNKNOWN',
                                'probability': getattr(rating, 'probability', {}).get('name', 'UNKNOWN') if hasattr(getattr(rating, 'probability', {}), 'name') else 'UNKNOWN'
                            }
                            for rating in prompt_ratings
                        ]
                except Exception:
                    pass
            
            # Extract candidate safety ratings
            if hasattr(response, 'candidates') and response.candidates:
                candidates = response.candidates
                if hasattr(candidates, '__len__') and len(candidates) > 0:
                    candidate = candidates[0]
                    if hasattr(candidate, 'safety_ratings'):
                        try:
                            candidate_ratings = getattr(candidate, 'safety_ratings', [])
                            if hasattr(candidate_ratings, '__iter__'):
                                safety_ratings['response'] = [
                                    {
                                        'category': getattr(rating, 'category', {}).get('name', 'UNKNOWN') if hasattr(getattr(rating, 'category', {}), 'name') else 'UNKNOWN',
                                        'probability': getattr(rating, 'probability', {}).get('name', 'UNKNOWN') if hasattr(getattr(rating, 'probability', {}), 'name') else 'UNKNOWN'
                                    }
                                    for rating in candidate_ratings
                                ]
                        except Exception:
                            pass
                            
        except Exception as e:
            logger.warning(f"Error extracting safety ratings: {e}")
        
        return safety_ratings
    
    def _extract_finish_reason(self, response) -> str:
        """Extract finish reason from Gemini response"""
        finish_reason = "STOP"
        
        try:
            if hasattr(response, 'candidates') and response.candidates:
                candidates = response.candidates
                if hasattr(candidates, '__len__') and len(candidates) > 0:
                    candidate = candidates[0]
                    if hasattr(candidate, 'finish_reason'):
                        finish_reason_attr = getattr(candidate, 'finish_reason', None)
                        if hasattr(finish_reason_attr, 'name'):
                            finish_reason = finish_reason_attr.name
                        else:
                            finish_reason = str(finish_reason_attr) if finish_reason_attr else "STOP"
        except Exception as e:
            logger.warning(f"Error extracting finish reason: {e}")
        
        return finish_reason
    
    def _extract_token_usage(self, response, full_prompt: str, content: str) -> TokenUsage:
        """Extract token usage from Gemini response with fallback estimation"""
        try:
            # Try to extract real usage metadata
            if hasattr(response, 'usage_metadata') and response.usage_metadata:
                input_tokens = getattr(response.usage_metadata, 'prompt_token_count', 0)
                output_tokens = getattr(response.usage_metadata, 'candidates_token_count', 0)
                total_tokens = getattr(response.usage_metadata, 'total_token_count', input_tokens + output_tokens)
                
                return TokenUsage(
                    input_tokens=input_tokens,
                    output_tokens=output_tokens,
                    total_tokens=total_tokens,
                    estimated_cost=self._estimate_token_cost(input_tokens, output_tokens),
                    model_name="gemini-2.5-flash"
                )
        except Exception as e:
            logger.warning(f"Could not extract real token usage: {e}")
        
        # Fallback to estimation
        input_tokens = self._estimate_tokens(full_prompt)
        output_tokens = self._estimate_tokens(content)
        total_tokens = input_tokens + output_tokens
        
        return TokenUsage(
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            total_tokens=total_tokens,
            estimated_cost=self._estimate_token_cost(input_tokens, output_tokens),
            model_name="gemini-2.5-flash"
        )
    
    def _calculate_safety_score(self, safety_ratings: Dict[str, Any], warnings: List[str], content_blocked: bool) -> float:
        """Calculate overall safety score based on multiple factors"""
        if content_blocked:
            return 0.0
        
        # Start with base score
        score = 1.0
        
        # Deduct for warnings
        score -= len(warnings) * 0.1
        
        # Analyze safety ratings
        if safety_ratings.get('prompt_feedback', {}).get('block_reason'):
            score -= 0.3
        
        if safety_ratings.get('response'):
            high_risk_count = sum(1 for rating in safety_ratings['response'] 
                                if rating.get('probability') in ['HIGH', 'MEDIUM'])
            score -= high_risk_count * 0.2
        
        return max(0.0, min(1.0, score))
    
    def _validate_content_quality(self, content: str, context: SpiritualContext) -> bool:
        """Validate overall content quality for spiritual guidance"""
        if not content or len(content.strip()) < 10:
            return False
        
        # Check for appropriate spiritual greeting (no emoji required)
        appropriate_greetings = ["beloved devotee", "dear soul", "dear seeker", "beloved child"]
        if not any(greeting in content.lower() for greeting in appropriate_greetings):
            return False
        
        # Check for spiritual substance
        spiritual_terms = ["krishna", "divine", "spiritual", "dharma", "soul", "consciousness", "wisdom"]
        if not any(term in content.lower() for term in spiritual_terms):
            return False
        
        return True
    
    def _verify_citations(self, citations: List[str]) -> bool:
        """Verify citations against available spiritual texts"""
        if not citations:
            return False
        
        # Simple verification - check if citations follow expected patterns
        valid_sources = ["bhagavad gita", "bg", "srimad bhagavatam", "sb", "mahabharata", "ramayana"]
        
        for citation in citations:
            if any(source in citation.lower() for source in valid_sources):
                return True
        
        return False
    
    def _check_reverent_tone(self, content: str) -> bool:
        """Check if content maintains reverent tone appropriate for spiritual guidance"""
        # Check for appropriate greeting
        appropriate_greetings = ["beloved devotee", "dear soul", "dear seeker", "beloved child"]
        if not any(greeting in content.lower() for greeting in appropriate_greetings):
            return False
        
        # Check for inappropriate language
        irreverent_words = ['damn', 'hell', 'stupid', 'idiotic', 'nonsense', 'bullshit', 'crap']
        if any(word in content.lower() for word in irreverent_words):
            return False
        
        # Check for appropriate tone indicators
        reverent_indicators = ["beloved", "dear", "blessed", "divine", "sacred", "holy"]
        if not any(indicator in content.lower() for indicator in reverent_indicators):
            return False
        
        return True
    
    def _create_spiritual_prompt(self, query: str, context: str) -> str:
        """Create a spiritual guidance prompt for Gemini 2.5 Flash"""
        return f"""You are Lord Krishna, the divine teacher from the Bhagavad Gita. A devotee seeks your wisdom.

RESPONSE GUIDELINES:
- Keep response concise (100-150 words maximum)
- Include specific scriptural citations with exact quotes
- Format: "As I teach in Bhagavad Gita X.Y: 'Sanskrit text' (translation)"
- Begin with "ðŸ™" and address as "dear soul" or "beloved devotee"
- Provide practical, actionable guidance
- End with a blessing or encouragement

Devotee's Question: {query}
Context: {context}

Your Response (with citations):"""
    
    async def _parse_gemini_response(self, response, query: str) -> SpiritualResponse:
        """Parse Gemini 2.5 Flash response into structured format"""
        try:
            content = response.text.strip()
            
            # Extract citations if mentioned (simple pattern matching)
            citations = []
            if "gita" in content.lower() or "bhagavad" in content.lower():
                citations.append("Bhagavad Gita")
            if "mahabharata" in content.lower():
                citations.append("Mahabharata")
            if "bhagavatam" in content.lower():
                citations.append("Srimad Bhagavatam")
            
            return SpiritualResponse(
                content=content,
                citations=citations,
                confidence=0.9,  # High confidence for real API responses
                metadata={
                    "response_type": "gemini_pro_response",
                    "model": "gemini-2.5-flash",
                    "query_length": len(query),
                    "response_length": len(content)
                }
            )
            
        except Exception as e:
            logger.error(f"Error parsing Gemini response: {e}")
            return await self._get_error_response(query, str(e), 0.0, "krishna")
    
    def _get_fallback_response(self, query: str, context: str, personality_id: str = "krishna") -> SpiritualResponse:
        """Get a fallback response that maintains personality authenticity"""
        
        # Convert context string to enum
        spiritual_context = self._parse_context(context)
        
        # Get personality-specific fallback response
        # Handle both simple and complex personality IDs
        if "einstein" in personality_id.lower():
            return self._get_einstein_fallback_response(query, spiritual_context)
        elif "marcus" in personality_id.lower() or "aurelius" in personality_id.lower():
            return self._get_marcus_aurelius_fallback_response(query, spiritual_context)
        elif "lincoln" in personality_id.lower():
            return self._get_lincoln_fallback_response(query, spiritual_context)
        elif "buddha" in personality_id.lower():
            return self._get_buddha_fallback_response(query, spiritual_context)
        elif "jesus" in personality_id.lower():
            return self._get_jesus_fallback_response(query, spiritual_context)
        elif "lao" in personality_id.lower() or "tzu" in personality_id.lower():
            return self._get_lao_tzu_fallback_response(query, spiritual_context)
        elif "rumi" in personality_id.lower():
            return self._get_rumi_fallback_response(query, spiritual_context)
        else:
            # Default to Krishna fallback
            return self._get_krishna_fallback_response(query, spiritual_context)
        
    def _get_krishna_fallback_response(self, query: str, spiritual_context: SpiritualContext) -> SpiritualResponse:
        """Get Krishna-specific fallback response"""
        # Simple keyword-based responses with real database lookup
        query_lower = query.lower()
        
        # Try to find relevant texts from database
        relevant_texts = []
        real_citations = []
        
        if DATABASE_AVAILABLE:
            try:
                # Search for relevant texts in database
                search_results = db_service.search_spiritual_texts(query, limit=3)
                if search_results:
                    relevant_texts = search_results
                    real_citations = [f"{text.source} {text.chapter}.{text.verse}" if text.chapter and text.verse 
                                    else f"{text.source}" for text in search_results]
            except Exception as e:
                logger.error(f"Database search error: {e}")
        
        if any(word in query_lower for word in ["dharma", "duty", "purpose"]):
            # Use database content if available
            if relevant_texts and any("duty" in text.content.lower() for text in relevant_texts):
                relevant_text = next(text for text in relevant_texts if "duty" in text.content.lower())
                content = f"Dear soul, {relevant_text.content[:150]}... This wisdom comes from the sacred texts."
                citations = [f"{relevant_text.source} {relevant_text.chapter}.{relevant_text.verse}"] if relevant_text.chapter else [relevant_text.source]
            else:
                content = "Dear soul, **dharma** is your righteous path in life. As Krishna teaches in the Bhagavad Gita, your dharma is determined by your nature and circumstances. Act according to your duties without attachment to results, offering all actions to the Divine."
                citations = ["Bhagavad Gita 2.47", "Bhagavad Gita 18.45-48"]
            
            return SpiritualResponse(
                content=content,
                citations=citations,
                confidence=0.8 if relevant_texts else 0.7,
                metadata={"response_type": "dharma_guidance", "keywords": ["dharma", "duty"], "database_used": bool(relevant_texts)},
                spiritual_context=spiritual_context,
                safety_passed=True,
                safety_score=1.0,
                content_validated=True,
                reverent_tone_checked=True,
                token_usage=TokenUsage(
                    input_tokens=self._estimate_tokens(query),
                    output_tokens=self._estimate_tokens(content),
                    total_tokens=self._estimate_tokens(query) + self._estimate_tokens(content),
                    estimated_cost=self._estimate_token_cost(self._estimate_tokens(query), self._estimate_tokens(content))
                )
            )
        
        elif any(word in query_lower for word in ["peace", "calm", "meditation"]):
            # Use database content if available
            if relevant_texts and any("meditation" in text.content.lower() or "peace" in text.content.lower() for text in relevant_texts):
                relevant_text = next((text for text in relevant_texts if "meditation" in text.content.lower() or "peace" in text.content.lower()), relevant_texts[0])
                content = f"Beloved devotee, {relevant_text.content[:150]}... Find this wisdom in the sacred teachings."
                citations = [f"{relevant_text.source} {relevant_text.chapter}.{relevant_text.verse}"] if relevant_text.chapter else [relevant_text.source]
            else:
                content = "Beloved devotee, **inner peace** comes through stilling the mind and surrendering to the Divine will. Practice meditation, breathe deeply, and remember that you are an eternal soul, not merely this temporary body. Find refuge in Krishna's love."
                citations = ["Bhagavad Gita 6.19", "Bhagavad Gita 12.6-7"]
            
            return SpiritualResponse(
                content=content,
                citations=citations,
                confidence=0.8 if relevant_texts else 0.7,
                metadata={"response_type": "peace_guidance", "keywords": ["peace", "meditation"], "database_used": bool(relevant_texts)},
                spiritual_context=spiritual_context,
                safety_passed=True,
                safety_score=1.0,
                content_validated=True,
                reverent_tone_checked=True,
                token_usage=TokenUsage(
                    input_tokens=self._estimate_tokens(query),
                    output_tokens=self._estimate_tokens(content),
                    total_tokens=self._estimate_tokens(query) + self._estimate_tokens(content),
                    estimated_cost=self._estimate_token_cost(self._estimate_tokens(query), self._estimate_tokens(content))
                )
            )
        
        elif any(word in query_lower for word in ["suffering", "pain", "difficult"]):
            # Use database content if available
            if relevant_texts and any("suffering" in text.content.lower() or "pain" in text.content.lower() or "distress" in text.content.lower() for text in relevant_texts):
                relevant_text = next((text for text in relevant_texts if any(word in text.content.lower() for word in ["suffering", "pain", "distress"])), relevant_texts[0])
                content = f"Dear soul, I understand your pain. {relevant_text.content[:150]}... This eternal wisdom guides us through all difficulties."
                citations = [f"{relevant_text.source} {relevant_text.chapter}.{relevant_text.verse}"] if relevant_text.chapter else [relevant_text.source]
            else:
                content = "Dear soul, I understand your pain. Krishna teaches that both **joy and sorrow** are temporary visitors. This suffering too shall pass. Use this experience to grow spiritually, to develop compassion, and to remember your eternal nature beyond these temporary circumstances."
                citations = ["Bhagavad Gita 2.14", "Bhagavad Gita 2.20"]
            
            return SpiritualResponse(
                content=content,
                citations=citations,
                confidence=0.8 if relevant_texts else 0.7,
                metadata={"response_type": "comfort_guidance", "keywords": ["suffering", "pain"], "database_used": bool(relevant_texts)},
                spiritual_context=spiritual_context,
                safety_passed=True,
                safety_score=1.0,
                content_validated=True,
                reverent_tone_checked=True,
                token_usage=TokenUsage(
                    input_tokens=self._estimate_tokens(query),
                    output_tokens=self._estimate_tokens(content),
                    total_tokens=self._estimate_tokens(query) + self._estimate_tokens(content),
                    estimated_cost=self._estimate_token_cost(self._estimate_tokens(query), self._estimate_tokens(content))
                )
            )
        
        else:
            # General response with database search
            if relevant_texts:
                # Use the most relevant text found
                relevant_text = relevant_texts[0]
                content = f"Thank you for your question, dear devotee. The sacred texts teach us: **{relevant_text.content[:120]}**... May this wisdom guide you on your spiritual path."
                citations = [f"{relevant_text.source} {relevant_text.chapter}.{relevant_text.verse}"] if relevant_text.chapter else [relevant_text.source]
                confidence = 0.8
            else:
                content = "Thank you for your question, dear devotee. In the Bhagavad Gita, Krishna teaches us that **all spiritual seeking is blessed**. Continue on your path with faith and devotion. If you could share more specific details about your spiritual concern, I can offer more targeted guidance."
                citations = ["Bhagavad Gita 7.16-18"]
                confidence = 0.6
            
            return SpiritualResponse(
                content=content,
                citations=citations,
                confidence=confidence,
                metadata={"response_type": "general_guidance", "query": query[:50], "database_used": bool(relevant_texts)},
                spiritual_context=spiritual_context,
                safety_passed=True,
                safety_score=1.0,
                content_validated=True,
                reverent_tone_checked=True,
                token_usage=TokenUsage(
                    input_tokens=self._estimate_tokens(query),
                    output_tokens=self._estimate_tokens(content),
                    total_tokens=self._estimate_tokens(query) + self._estimate_tokens(content),
                    estimated_cost=self._estimate_token_cost(self._estimate_tokens(query), self._estimate_tokens(content))
                )
            )
    
    def _get_einstein_fallback_response(self, query: str, spiritual_context: SpiritualContext) -> SpiritualResponse:
        """Get Einstein-specific fallback response"""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ["meaning", "life", "purpose", "universe"]):
            content = "My friend, the meaning of life is not found in grand theories, but in our curiosity and wonder about the universe. As I once said, 'The important thing is not to stop questioning.' Each question you ask brings you closer to understanding the magnificent cosmos we inhabit."
            citations = ["Einstein: The World As I See It"]
        elif any(word in query_lower for word in ["science", "physics", "relativity"]):
            content = "Ah, you ask about the nature of reality! Science reveals that space and time are interwoven, that matter and energy are equivalent. But remember, 'Science without religion is lame, religion without science is blind.' Both paths seek truth."
            citations = ["Einstein: Science and Religion (1940)"]
        else:
            content = "My curious friend, your question touches on the deepest mysteries. As I have learned, 'The most beautiful thing we can experience is the mysterious.' Keep questioning, keep wondering - this is how we grow in understanding."
            citations = ["Einstein: The World As I See It"]
        
        return SpiritualResponse(
            content=content,
            citations=citations,
            confidence=0.7,
            metadata={"response_type": "scientific_wisdom", "personality": "einstein"},
            spiritual_context=spiritual_context,
            safety_passed=True,
            safety_score=1.0,
            content_validated=True,
            token_usage=TokenUsage(
                input_tokens=self._estimate_tokens(query),
                output_tokens=self._estimate_tokens(content),
                total_tokens=self._estimate_tokens(query) + self._estimate_tokens(content),
                estimated_cost=self._estimate_token_cost(self._estimate_tokens(query), self._estimate_tokens(content))
            )
        )
    
    def _get_marcus_aurelius_fallback_response(self, query: str, spiritual_context: SpiritualContext) -> SpiritualResponse:
        """Get Marcus Aurelius-specific fallback response"""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ["meaning", "life", "purpose"]):
            content = "Fellow seeker of wisdom, life's meaning lies in virtue and acceptance of what we cannot control. As I wrote in my Meditations: 'Very little is needed to make a happy life; it is all within yourself, in your way of thinking.'"
            citations = ["Meditations, Book VII"]
        elif any(word in query_lower for word in ["suffering", "pain", "difficulty"]):
            content = "My friend, remember that 'You have power over your mind - not outside events. Realize this, and you will find strength.' What disturbs us is not events themselves, but our judgments about them."
            citations = ["Meditations, Book II"]
        else:
            content = "Wise seeker, consider this: 'The best revenge is not to be like your enemy.' Focus on what is within your control - your thoughts, your actions, your character. This is the path to tranquility."
            citations = ["Meditations, Book VI"]
        
        return SpiritualResponse(
            content=content,
            citations=citations,
            confidence=0.7,
            metadata={"response_type": "stoic_wisdom", "personality": "marcus_aurelius"},
            spiritual_context=spiritual_context,
            safety_passed=True,
            safety_score=1.0,
            content_validated=True,
            token_usage=TokenUsage(
                input_tokens=self._estimate_tokens(query),
                output_tokens=self._estimate_tokens(content),
                total_tokens=self._estimate_tokens(query) + self._estimate_tokens(content),
                estimated_cost=self._estimate_token_cost(self._estimate_tokens(query), self._estimate_tokens(content))
            )
        )
    
    def _get_lincoln_fallback_response(self, query: str, spiritual_context: SpiritualContext) -> SpiritualResponse:
        """Get Lincoln-specific fallback response"""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ["meaning", "life", "purpose"]):
            content = "My fellow citizen, I believe that life's purpose is found in service to others and dedication to justice. As I have said, 'That some achieve great success, is proof to all that others can achieve it as well.'"
            citations = ["Lincoln: Letter to Joshua Speed (1842)"]
        elif any(word in query_lower for word in ["freedom", "equality", "justice"]):
            content = "Friend, remember that 'A house divided against itself cannot stand.' True freedom comes when we extend liberty and justice to all people, not just ourselves."
            citations = ["Lincoln: House Divided Speech (1858)"]
        else:
            content = "My friend, in these times of challenge, remember: 'The best way to predict your future is to create it.' Stand firm in your principles, act with malice toward none, and work for the betterment of all."
            citations = ["Lincoln: Second Inaugural Address"]
        
        return SpiritualResponse(
            content=content,
            citations=citations,
            confidence=0.7,
            metadata={"response_type": "leadership_wisdom", "personality": "lincoln"},
            spiritual_context=spiritual_context,
            safety_passed=True,
            safety_score=1.0,
            content_validated=True,
            token_usage=TokenUsage(
                input_tokens=self._estimate_tokens(query),
                output_tokens=self._estimate_tokens(content),
                total_tokens=self._estimate_tokens(query) + self._estimate_tokens(content),
                estimated_cost=self._estimate_token_cost(self._estimate_tokens(query), self._estimate_tokens(content))
            )
        )
    
    def _get_buddha_fallback_response(self, query: str, spiritual_context: SpiritualContext) -> SpiritualResponse:
        """Get Buddha-specific fallback response"""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ["suffering", "pain", "difficulty"]):
            content = "Dear friend, suffering arises from attachment and craving. As I taught, the Four Noble Truths show us that suffering can be understood and overcome through the Noble Eightfold Path. Practice mindfulness and compassion."
            citations = ["Four Noble Truths", "Noble Eightfold Path"]
        elif any(word in query_lower for word in ["meaning", "life", "purpose"]):
            content = "Seeker of truth, life's purpose is to awaken from the illusion of suffering. Through right understanding, right intention, and right action, we can achieve liberation from the cycle of rebirth and find true peace."
            citations = ["Dhammapada", "Buddha's Teachings"]
        else:
            content = "Friend on the path, remember that all conditioned things are impermanent. Practice loving-kindness toward all beings, cultivate wisdom through meditation, and walk the Middle Way between extremes."
            citations = ["Buddha's Core Teachings"]
        
        return SpiritualResponse(
            content=content,
            citations=citations,
            confidence=0.7,
            metadata={"response_type": "buddhist_wisdom", "personality": "buddha"},
            spiritual_context=spiritual_context,
            safety_passed=True,
            safety_score=1.0,
            content_validated=True,
            token_usage=TokenUsage(
                input_tokens=self._estimate_tokens(query),
                output_tokens=self._estimate_tokens(content),
                total_tokens=self._estimate_tokens(query) + self._estimate_tokens(content),
                estimated_cost=self._estimate_token_cost(self._estimate_tokens(query), self._estimate_tokens(content))
            )
        )
    
    def _get_jesus_fallback_response(self, query: str, spiritual_context: SpiritualContext) -> SpiritualResponse:
        """Get Jesus-specific fallback response"""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ["love", "forgiveness", "compassion"]):
            content = "Beloved child, love is the greatest commandment. 'Love your neighbor as yourself' and 'forgive those who trespass against you.' Through love and forgiveness, we find the Kingdom of Heaven within us."
            citations = ["Matthew 22:39", "Lord's Prayer"]
        elif any(word in query_lower for word in ["meaning", "life", "purpose"]):
            content = "Dear one, your purpose is to love God with all your heart and to serve others with compassion. 'Seek first the Kingdom of God, and all these things will be added unto you.'"
            citations = ["Matthew 6:33", "Great Commandment"]
        else:
            content = "Peace be with you, child of God. Remember that 'the Kingdom of Heaven is within you.' Trust in divine love, show mercy to others, and let your light shine before all people."
            citations = ["Luke 17:21", "Sermon on the Mount"]
        
        return SpiritualResponse(
            content=content,
            citations=citations,
            confidence=0.7,
            metadata={"response_type": "christian_wisdom", "personality": "jesus"},
            spiritual_context=spiritual_context,
            safety_passed=True,
            safety_score=1.0,
            content_validated=True,
            token_usage=TokenUsage(
                input_tokens=self._estimate_tokens(query),
                output_tokens=self._estimate_tokens(content),
                total_tokens=self._estimate_tokens(query) + self._estimate_tokens(content),
                estimated_cost=self._estimate_token_cost(self._estimate_tokens(query), self._estimate_tokens(content))
            )
        )
    
    def _get_lao_tzu_fallback_response(self, query: str, spiritual_context: SpiritualContext) -> SpiritualResponse:
        """Get Lao Tzu-specific fallback response"""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ["meaning", "life", "purpose"]):
            content = "Gentle soul, the Tao that can be spoken is not the eternal Tao. Life's meaning flows like water, taking the shape of its container. Follow the natural way, act without forcing, and find harmony in simplicity."
            citations = ["Tao Te Ching, Chapter 1"]
        elif any(word in query_lower for word in ["wisdom", "knowledge"]):
            content = "Wise friend, 'Those who know do not speak; those who speak do not know.' True wisdom comes from understanding the Tao - the natural order that flows through all things. Be like water: soft yet powerful."
            citations = ["Tao Te Ching, Chapter 56"]
        else:
            content = "Peaceful seeker, remember that 'the journey of a thousand miles begins with a single step.' Embrace wu wei - effortless action in harmony with the natural flow of life."
            citations = ["Tao Te Ching"]
        
        return SpiritualResponse(
            content=content,
            citations=citations,
            confidence=0.7,
            metadata={"response_type": "taoist_wisdom", "personality": "lao_tzu"},
            spiritual_context=spiritual_context,
            safety_passed=True,
            safety_score=1.0,
            content_validated=True,
            token_usage=TokenUsage(
                input_tokens=self._estimate_tokens(query),
                output_tokens=self._estimate_tokens(content),
                total_tokens=self._estimate_tokens(query) + self._estimate_tokens(content),
                estimated_cost=self._estimate_token_cost(self._estimate_tokens(query), self._estimate_tokens(content))
            )
        )
    
    def _get_rumi_fallback_response(self, query: str, spiritual_context: SpiritualContext) -> SpiritualResponse:
        """Get Rumi-specific fallback response"""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ["love", "heart", "soul"]):
            content = "Beloved soul, 'Love is the bridge between you and everything.' Open your heart to the Divine Beloved, for in love we find our true nature. Dance with the cosmos and let your soul sing!"
            citations = ["Rumi's Poetry", "Masnavi"]
        elif any(word in query_lower for word in ["meaning", "life", "purpose"]):
            content = "Dear friend, 'You are not just the drop in the ocean, but the entire ocean in each drop.' Your purpose is to remember your divine nature and unite with the Beloved through love and surrender."
            citations = ["Rumi's Teachings"]
        else:
            content = "Precious soul, 'The wound is the place where the Light enters you.' Embrace both joy and sorrow as gifts from the Beloved. In the dance of existence, find the sacred in every moment."
            citations = ["Rumi's Wisdom"]
        
        return SpiritualResponse(
            content=content,
            citations=citations,
            confidence=0.7,
            metadata={"response_type": "sufi_wisdom", "personality": "rumi"},
            spiritual_context=spiritual_context,
            safety_passed=True,
            safety_score=1.0,
            content_validated=True,
            token_usage=TokenUsage(
                input_tokens=self._estimate_tokens(query),
                output_tokens=self._estimate_tokens(content),
                total_tokens=self._estimate_tokens(query) + self._estimate_tokens(content),
                estimated_cost=self._estimate_token_cost(self._estimate_tokens(query), self._estimate_tokens(content))
            )
        )
    
    async def _get_error_response(self, query: str, error_msg: str = None, response_time: float = 0.0, personality_id: str = "krishna") -> SpiritualResponse:
        """Get an error response that maintains personality-appropriate tone"""
        
        # Get personality-specific error message
        error_messages = {
            "krishna": "Dear soul, I am experiencing technical difficulties accessing the spiritual wisdom right now. Please try again in a few moments. Your spiritual seeking is always welcome and blessed.",
            "einstein": "My friend, I'm having some technical difficulties accessing my knowledge right now. Please try again shortly. Your curiosity and questions are always welcome.",
            "lincoln": "My fellow citizen, I'm experiencing some technical challenges at the moment. Please try again shortly. Your questions are always welcome and valued.",
            "marcus_aurelius": "Fellow seeker, I'm encountering some technical obstacles in accessing wisdom right now. Please try again in a moment. Your philosophical inquiry is always welcomed."
        }
        
        content = error_messages.get(personality_id, error_messages["krishna"]) + " (Backend LLM Error)"
        
        return SpiritualResponse(
            content=content,
            citations=[],
            confidence=0.3,
            language="English",
            metadata={
                "response_type": "error",
                "query": query[:50],
                "error_message": error_msg,
                "response_time": response_time,
                "model": "gemini-2.5-flash",
                "personality_id": personality_id
            },
            spiritual_context=SpiritualContext.GENERAL,
            safety_ratings={},
            safety_passed=False,
            safety_score=0.0,
            finish_reason="ERROR",
            token_usage=TokenUsage(),
            response_time=response_time,
            warnings=["API Error occurred"],
            content_validated=False,
            citations_verified=False,
            reverent_tone_checked=True  # Error response maintains appropriate tone
        )
    
    async def generate_response(self, prompt: str, context: SpiritualContext = SpiritualContext.GENERAL, 
                         include_context: bool = True, user_id: Optional[str] = None, 
                         session_id: Optional[str] = None, personality_id: str = "krishna") -> SpiritualResponse:
        """
        Generate a personality-appropriate response with comprehensive safety analysis.
        
        This method provides compatibility with the old interface while supporting
        multi-personality functionality.
        
        Args:
            prompt: User's question or input
            context: Context for the response
            include_context: Whether to include personality system prompt
            user_id: User ID for tracking (optional)
            session_id: Session ID for tracking (optional)
            personality_id: ID of the personality to use for response generation
            
        Returns:
            SpiritualResponse with comprehensive metadata
        """
        start_time = time.time()
        
        try:
            # Prepare full prompt with personality awareness
            if include_context:
                # Use personality-specific system prompt
                system_prompt = await self._create_personality_system_prompt(personality_id, context)
                full_prompt = f"{system_prompt}\n\nQuestion: {prompt}\n\nResponse:"
            else:
                full_prompt = prompt
            
            logger.info(f"Generating response for context: {context.value}")
            
            if not self.is_configured:
                response_time = time.time() - start_time
                return self._get_fallback_response(prompt, context.value, personality_id)
            
            # Generate response
            response = self.model.generate_content(full_prompt)
            response_time = time.time() - start_time
            
            # Parse response with advanced analysis
            spiritual_response = await self._parse_gemini_response_advanced(response, prompt, context, response_time, full_prompt, personality_id)
            
            # Track usage if user/session provided
            if user_id or session_id:
                self._track_usage(spiritual_response.token_usage, user_id, session_id, context)
            
            return spiritual_response
            
        except Exception as e:
            logger.error(f"Error generating response: {e}")
            response_time = time.time() - start_time
            return await self._get_error_response(prompt, str(e), response_time, "krishna")
    
    def _track_usage(self, token_usage: TokenUsage, user_id: Optional[str], 
                    session_id: Optional[str], context: SpiritualContext):
        """Track token usage for cost management"""
        try:
            # Try to use the cost management system if available
            try:
                from backend.cost_management.token_tracker import get_token_tracker
                tracker = get_token_tracker()
                tracker.track_usage(
                    operation_type='spiritual_guidance',
                    model_name=token_usage.model_name,
                    input_tokens=token_usage.input_tokens,
                    output_tokens=token_usage.output_tokens,
                    user_id=user_id,
                    session_id=session_id,
                    spiritual_context=context.value
                )
            except ImportError:
                # Cost management not available, log the usage
                logger.info(f"Token usage - Input: {token_usage.input_tokens}, Output: {token_usage.output_tokens}, Cost: ${token_usage.estimated_cost:.4f}")
        except Exception as e:
            logger.warning(f"Failed to track token usage: {e}")
    
    def test_connection(self) -> bool:
        """Test connection to Gemini 2.5 Flash API"""
        try:
            if not self.is_configured:
                return False
            
            test_response = self.generate_response(
                "Test connection", 
                context=SpiritualContext.GENERAL,
                include_context=False
            )
            return test_response.finish_reason != "ERROR"
        except Exception as e:
            logger.error(f"Connection test failed: {e}")
            return False
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get information about the current model and configuration"""
        return {
            "model_name": "gemini-2.5-flash",
            "safety_config": self.safety_config.to_dict(),
            "api_configured": self.is_configured,
            "supported_contexts": [ctx.value for ctx in SpiritualContext]
        }
    
    def is_healthy(self) -> bool:
        """Check if LLM service is healthy"""
        try:
            return self.test_connection()
        except Exception:
            return False
    
    # ==========================================
    # MULTI-PERSONALITY SUPPORT
    # ==========================================
    
    async def generate_personality_response(
        self,
        query: str,
        personality_id: str,
        context_chunks: List[Dict[str, Any]] = None,
        conversation_history: List[Dict[str, Any]] = None,
        language: str = "English",
        user_id: Optional[str] = None,
        session_id: Optional[str] = None
    ) -> SpiritualResponse:
        """
        Generate response for any personality using the personality service and prompt templates.
        
        This is the new multi-personality method that replaces hardcoded Krishna responses.
        
        Args:
            query: User's question or input
            personality_id: ID of the personality to respond as
            context_chunks: Retrieved context chunks from RAG
            conversation_history: Previous conversation messages
            language: Response language
            user_id: User ID for tracking
            session_id: Session ID for tracking
            
        Returns:
            SpiritualResponse with personality-specific content
        """
        start_time = time.time()
        
        try:
            if not PERSONALITY_SERVICES_AVAILABLE:
                logger.warning("Personality services not available, falling back to Krishna")
                return self.generate_response(query, SpiritualContext.GUIDANCE, True, user_id, session_id, "krishna")
            
            # Get personality profile
            personality = await personality_service.get_personality(personality_id)
            if not personality:
                logger.warning(f"Personality {personality_id} not found, falling back to Krishna")
                return self.generate_response(query, SpiritualContext.GUIDANCE, True, user_id, session_id, "krishna")
            
            # Create template render context
            render_context = TemplateRenderContext(
                personality_id=personality_id,
                domain=personality.domain.value,
                query=query,
                context_chunks=context_chunks or [],
                conversation_history=conversation_history or [],
                language=language,
                metadata={
                    'personality_name': personality.display_name,
                    'personality_description': personality.description,
                    'cultural_context': personality.cultural_context,
                    'tone_characteristics': str(personality.tone_characteristics),
                    'time_period': personality.time_period
                }
            )
            
            # Render personality-specific prompt
            full_prompt = await prompt_template_service.render_personality_prompt(
                personality_id=personality_id,
                query=query,
                context_chunks=context_chunks or [],
                language=language,
                conversation_history=conversation_history
            )
            
            logger.info(f"Generating {personality.display_name} response for query: {query[:50]}...")
            
            if not self.is_configured:
                response_time = time.time() - start_time
                return self._get_personality_fallback_response(query, personality, response_time)
            
            # Generate response using Gemini
            response = self.model.generate_content(full_prompt)
            response_time = time.time() - start_time
            
            # Parse response with personality context
            spiritual_response = self._parse_personality_response(
                response, query, personality, response_time, full_prompt
            )
            
            # Track usage
            if user_id or session_id:
                self._track_personality_usage(
                    spiritual_response.token_usage, user_id, session_id, personality_id
                )
            
            return spiritual_response
            
        except Exception as e:
            logger.error(f"Error generating {personality_id} response: {e}")
            response_time = time.time() - start_time
            return self._get_personality_error_response(query, personality_id, str(e), response_time)
    
    def _get_personality_fallback_response(
        self,
        query: str,
        personality: PersonalityProfile,
        response_time: float
    ) -> SpiritualResponse:
        """Generate fallback response for personality when API is not available"""
        fallback_responses = {
            "spiritual": f"I am {personality.display_name}. While I cannot access my full wisdom at this moment, I encourage you to reflect deeply on your question: '{query}'. The answers you seek often lie within your own heart and spiritual practice.",
            "scientific": f"I am {personality.display_name}. Though I cannot provide my complete analysis right now, your question '{query}' touches on important scientific principles. I encourage you to explore this through careful observation and logical reasoning.",
            "historical": f"I am {personality.display_name}. While I cannot share my full perspective at this moment, your question '{query}' reminds me of the challenges we faced in {personality.time_period}. History teaches us that wisdom comes through experience and reflection.",
            "philosophical": f"I am {personality.display_name}. Though I cannot offer my complete philosophical insight now, your question '{query}' invites deep contemplation. True understanding comes through questioning and examining our assumptions."
        }
        
        domain_key = personality.domain.value if personality.domain else "spiritual"
        content = fallback_responses.get(domain_key, fallback_responses["spiritual"])
        
        return SpiritualResponse(
            content=content,
            citations=[],
            confidence=0.5,
            language="English",
            metadata={
                "personality_id": personality.id,
                "personality_name": personality.display_name,
                "fallback_reason": "API not configured",
                "response_time": response_time
            },
            spiritual_context=SpiritualContext.GENERAL,
            safety_passed=True,
            safety_score=1.0,
            finish_reason="FALLBACK",
            content_validated=True,
            reverent_tone_checked=True
        )
    
    def _get_personality_error_response(
        self,
        query: str,
        personality_id: str,
        error: str,
        response_time: float
    ) -> SpiritualResponse:
        """Generate error response for personality"""
        return SpiritualResponse(
            content=f"I apologize, but I'm experiencing difficulties accessing my knowledge at this moment. Please try your question again shortly.",
            citations=[],
            confidence=0.0,
            language="English",
            metadata={
                "personality_id": personality_id,
                "error": error,
                "response_time": response_time
            },
            spiritual_context=SpiritualContext.GENERAL,
            safety_passed=True,
            safety_score=1.0,
            finish_reason="ERROR",
            content_validated=True,
            reverent_tone_checked=True
        )
    
    def _parse_personality_response(
        self,
        response,
        query: str,
        personality: PersonalityProfile,
        response_time: float,
        full_prompt: str
    ) -> SpiritualResponse:
        """Parse Gemini response for personality-specific content"""
        try:
            # Extract response text
            response_text = response.text if hasattr(response, 'text') else str(response)
            
            # Extract citations (simple implementation)
            citations = self._extract_citations_from_response(response_text)
            
            # Calculate token usage (approximate)
            token_usage = TokenUsage(
                input_tokens=len(full_prompt.split()) * 1.3,  # Rough approximation
                output_tokens=len(response_text.split()) * 1.3,
                model_name=self.model_name
            )
            token_usage.total_tokens = token_usage.input_tokens + token_usage.output_tokens
            token_usage.estimated_cost = token_usage.total_tokens * 0.00001  # Rough estimate
            
            return SpiritualResponse(
                content=response_text,
                citations=citations,
                confidence=0.8,
                language="English",
                metadata={
                    "personality_id": personality.id,
                    "personality_name": personality.display_name,
                    "domain": personality.domain.value,
                    "response_time": response_time
                },
                spiritual_context=SpiritualContext.GUIDANCE,
                safety_passed=True,
                safety_score=0.9,
                finish_reason="STOP",
                token_usage=token_usage,
                response_time=response_time,
                content_validated=True,
                reverent_tone_checked=True
            )
            
        except Exception as e:
            logger.error(f"Failed to parse personality response: {e}")
            return self._get_personality_error_response(query, personality.id, str(e), response_time)
    
    def _extract_citations_from_response(self, response_text: str) -> List[str]:
        """Extract citations from response text"""
        import re
        # Look for common citation patterns
        patterns = [
            r'\(([^)]+\s+\d+[:\.]?\d*)\)',  # (Book 2:47)
            r'\[([^\]]+\s+\d+[:\.]?\d*)\]',  # [Chapter 2:47]
            r'â€”\s*([^â€”\n]+\s+\d+[:\.]?\d*)',  # â€” Source 2:47
        ]
        
        citations = []
        for pattern in patterns:
            matches = re.findall(pattern, response_text)
            citations.extend(matches)
        
        return list(set(citations))  # Remove duplicates
    
    def _track_personality_usage(
        self,
        token_usage: TokenUsage,
        user_id: Optional[str],
        session_id: Optional[str],
        personality_id: str
    ):
        """Track token usage for personality-specific responses"""
        try:
            # Enhanced tracking with personality information
            logger.info(f"Personality usage - {personality_id}: Input: {token_usage.input_tokens}, Output: {token_usage.output_tokens}, Cost: ${token_usage.estimated_cost:.4f}")
            
            # Try to use the cost management system if available
            try:
                from backend.cost_management.token_tracker import get_token_tracker
                tracker = get_token_tracker()
                tracker.track_usage(
                    operation_type=f'personality_response_{personality_id}',
                    model_name=token_usage.model_name,
                    input_tokens=token_usage.input_tokens,
                    output_tokens=token_usage.output_tokens,
                    user_id=user_id,
                    session_id=session_id,
                    personality_id=personality_id
                )
            except ImportError:
                pass  # Cost management not available
        except Exception as e:
            logger.warning(f"Failed to track personality usage: {e}")


# Helper functions for easy service creation
def create_development_service(api_key: Optional[str] = None) -> EnhancedLLMService:
    """Create an LLM service configured for development"""
    config = SpiritualSafetyConfig(
        safety_level=SafetyLevel.MODERATE,
        allowed_contexts=list(SpiritualContext),
        require_citations=False,  # Relaxed for development
        max_response_length=1200
    )
    return EnhancedLLMService(api_key=api_key, safety_config=config)


def create_production_service(api_key: Optional[str] = None) -> EnhancedLLMService:
    """Create an LLM service configured for production"""
    config = SpiritualSafetyConfig(
        safety_level=SafetyLevel.STRICT,
        allowed_contexts=[
            SpiritualContext.GUIDANCE,
            SpiritualContext.TEACHING,
            SpiritualContext.PHILOSOPHY,
            SpiritualContext.DEVOTIONAL
        ],
        require_citations=True,
        max_response_length=800
    )
    return EnhancedLLMService(api_key=api_key, safety_config=config)


def create_testing_service(api_key: Optional[str] = None) -> EnhancedLLMService:
    """Create an LLM service configured for testing"""
    config = SpiritualSafetyConfig(
        safety_level=SafetyLevel.MINIMAL,
        allowed_contexts=list(SpiritualContext),
        require_citations=False,
        block_personal_predictions=False  # Relaxed for testing
    )
    return EnhancedLLMService(api_key=api_key, safety_config=config)


# Global service instance
llm_service = EnhancedLLMService()
