#!/usr/bin/env python3
"""
Comprehensive Production Validation for Vimarsh
Tests all production components with real integration tests
"""

import asyncio
import json
import os
import sys
import logging
import time
import requests
from typing import Dict, Any, List
import subprocess
from pathlib import Path

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ProductionValidator:
    """Comprehensive production validation suite."""
    
    def __init__(self, base_url: str = "http://localhost:7071"):
        self.base_url = base_url
        self.test_results = []
        self.workspace_path = Path(__file__).parent.parent
        
    def log_test_result(self, test_name: str, passed: bool, details: str = "", data: Any = None):
        """Log and store test results."""
        status = "‚úÖ PASS" if passed else "‚ùå FAIL"
        logger.info(f"{status} - {test_name}: {details}")
        self.test_results.append({
            "test": test_name,
            "passed": passed,
            "details": details,
            "data": data,
            "timestamp": time.time()
        })
        
    def print_banner(self):
        """Print validation banner."""
        print("""
üïâÔ∏è  VIMARSH PRODUCTION VALIDATOR
==================================
Comprehensive End-to-End Testing

Testing Components:
ü§ñ 1. Gemini Pro LLM Integration
üìö 2. RAG Pipeline & Vector Search  
üîê 3. Authentication System
üéØ 4. Citation & Response Quality
üöÄ 5. Performance & Load Testing
""")

    async def test_environment_config(self) -> bool:
        """Test environment configuration completeness."""
        logger.info("üîß Testing Environment Configuration...")
        
        try:
            # Check backend config
            backend_config_path = self.workspace_path / "backend" / "local.settings.json"
            if not backend_config_path.exists():
                self.log_test_result("Backend Config", False, "local.settings.json not found")
                return False
                
            with open(backend_config_path) as f:
                config = json.load(f)
                
            required_keys = [
                "GOOGLE_AI_API_KEY", "AZURE_COSMOS_CONNECTION_STRING", 
                "AZURE_CLIENT_ID", "LLM_MODEL", "EMBEDDING_MODEL"
            ]
            
            # Development mode acceptable values
            dev_mode_values = {
                "GOOGLE_AI_API_KEY": ["dev-mode-placeholder"],
                "AZURE_COSMOS_CONNECTION_STRING": ["dev-mode-local-storage"],
                "AZURE_CLIENT_ID": ["dev-mode-disabled"]
            }
            
            missing_keys = []
            placeholder_keys = []
            
            for key in required_keys:
                value = config["Values"].get(key, "")
                if not value:
                    missing_keys.append(key)
                elif ("YOUR_" in value or "PLACEHOLDER" in value) and value not in dev_mode_values.get(key, []):
                    placeholder_keys.append(key)
                    
            if missing_keys:
                self.log_test_result("Environment Config", False, f"Missing keys: {missing_keys}")
                return False
                
            # Allow development mode with warnings
            cosmos_conn = config["Values"].get("AZURE_COSMOS_CONNECTION_STRING", "")
            if placeholder_keys:
                self.log_test_result("Environment Config", True, f"Development mode (placeholder values: {placeholder_keys})")
            elif cosmos_conn == "dev-mode-local-storage":
                self.log_test_result("Environment Config", True, "Development mode with local storage")
            else:
                self.log_test_result("Environment Config", True, "Production configuration detected")
                
            # Check frontend config
            frontend_env_path = self.workspace_path / "frontend" / ".env.development"
            if not frontend_env_path.exists():
                self.log_test_result("Frontend Config", False, ".env.development not found")
                return False
                
            self.log_test_result("Environment Config", True, "All configurations present")
            return True
            
        except Exception as e:
            self.log_test_result("Environment Config", False, f"Config validation error: {e}")
            return False

    async def test_gemini_integration(self) -> bool:
        """Test real Gemini Pro API integration."""
        logger.info("ü§ñ Testing Gemini Pro LLM Integration...")
        
        try:
            # Test direct API call
            test_payload = {
                "query": "What is dharma according to Hindu philosophy?",
                "language": "English"
            }
            
            response = requests.post(
                f"{self.base_url}/api/spiritual_guidance",
                json=test_payload,
                timeout=30
            )
            
            if response.status_code != 200:
                self.log_test_result("Gemini API Call", False, f"HTTP {response.status_code}")
                return False
                
            data = response.json()
            
            # Check response structure
            if "response" not in data:
                self.log_test_result("Gemini Response Structure", False, "Missing response field")
                return False
                
            guidance_text = data["response"]
            
            # Quality checks
            if len(guidance_text) < 50:
                self.log_test_result("Gemini Response Quality", False, "Response too short")
                return False
                
            # Check for Krishna persona
            krishna_indicators = ["Krishna", "dharma", "spiritual", "divine", "wisdom"]
            has_krishna_persona = any(indicator.lower() in guidance_text.lower() for indicator in krishna_indicators)
            
            if not has_krishna_persona:
                self.log_test_result("Krishna Persona", False, "Response lacks spiritual context")
                return False
                
            self.log_test_result("Gemini Integration", True, f"Response length: {len(guidance_text)} chars")
            return True
            
        except requests.exceptions.RequestException as e:
            self.log_test_result("Gemini API Connection", False, f"Connection error: {e}")
            return False
        except Exception as e:
            self.log_test_result("Gemini Integration", False, f"Unexpected error: {e}")
            return False

    async def test_rag_pipeline(self) -> bool:
        """Test RAG pipeline and vector search functionality."""
        logger.info("üìö Testing RAG Pipeline & Vector Search...")
        
        try:
            # Test with specific spiritual query
            test_payload = {
                "query": "What does the Bhagavad Gita say about karma yoga?",
                "language": "English"
            }
            
            response = requests.post(
                f"{self.base_url}/api/spiritual_guidance",
                json=test_payload,
                timeout=30
            )
            
            if response.status_code != 200:
                self.log_test_result("RAG API Call", False, f"HTTP {response.status_code}")
                return False
                
            data = response.json()
            
            # Check for citations
            if "citations" not in data:
                self.log_test_result("RAG Citations", False, "Missing citations field")
                return False
                
            citations = data["citations"]
            
            if not citations or len(citations) == 0:
                self.log_test_result("RAG Citation Content", False, "No citations returned")
                return False
                
            # Validate citation structure
            required_citation_fields = ["source", "text"]
            confidence_fields = ["confidence", "relevance_score"]  # Accept either
            
            for citation in citations[:3]:  # Check first 3 citations
                missing_fields = [field for field in required_citation_fields if field not in citation]
                
                # Check if at least one confidence-type field exists
                has_confidence = any(field in citation for field in confidence_fields)
                if not has_confidence:
                    missing_fields.append("confidence/relevance_score")
                
                if missing_fields:
                    self.log_test_result("Citation Structure", False, f"Missing fields: {missing_fields}")
                    return False
                    
            # Check relevance
            guidance_text = data["response"].lower()
            citation_texts = [c["text"].lower() for c in citations]
            
            # Look for relevant terms in citations
            relevant_terms = ["karma", "yoga", "action", "duty", "dharma"]
            citation_relevance = any(
                term in citation_text 
                for term in relevant_terms 
                for citation_text in citation_texts
            )
            
            if not citation_relevance:
                self.log_test_result("RAG Relevance", False, "Citations not relevant to query")
                return False
                
            self.log_test_result("RAG Pipeline", True, f"Found {len(citations)} relevant citations")
            return True
            
        except Exception as e:
            self.log_test_result("RAG Pipeline", False, f"Error: {e}")
            return False

    async def test_authentication_system(self) -> bool:
        """Test authentication system functionality."""
        logger.info("üîê Testing Authentication System...")
        
        try:
            # Check if auth is enabled
            backend_config_path = self.workspace_path / "backend" / "local.settings.json"
            with open(backend_config_path) as f:
                config = json.load(f)
                
            client_id = config["Values"].get("AZURE_CLIENT_ID", "")
            
            if client_id == "dev-mode":
                self.log_test_result("Authentication Mode", True, "Development mode (auth disabled)")
                return True
                
            # Test auth endpoint
            try:
                auth_response = requests.get(f"{self.base_url}/api/auth/status", timeout=10)
                if auth_response.status_code == 200:
                    self.log_test_result("Auth Endpoint", True, "Auth status endpoint responding")
                else:
                    self.log_test_result("Auth Endpoint", False, f"HTTP {auth_response.status_code}")
                    return False
            except requests.exceptions.RequestException:
                self.log_test_result("Auth Endpoint", False, "Auth endpoint not accessible")
                return False
                
            # Test protected endpoint without auth
            try:
                unauth_response = requests.post(
                    f"{self.base_url}/api/spiritual_guidance",
                    json={"query": "test"},
                    timeout=10
                )
                
                # Should either work (if auth disabled) or return 401
                if unauth_response.status_code in [200, 401]:
                    self.log_test_result("Auth Protection", True, f"Protected endpoint status: {unauth_response.status_code}")
                else:
                    self.log_test_result("Auth Protection", False, f"Unexpected status: {unauth_response.status_code}")
                    return False
                    
            except requests.exceptions.RequestException as e:
                self.log_test_result("Auth Protection", False, f"Error testing protection: {e}")
                return False
                
            self.log_test_result("Authentication System", True, "Auth system properly configured")
            return True
            
        except Exception as e:
            self.log_test_result("Authentication System", False, f"Error: {e}")
            return False

    async def test_sacred_text_database(self) -> bool:
        """Test sacred text database population and access."""
        logger.info("üìñ Testing Sacred Text Database...")
        
        try:
            # Check if sacred texts data exists
            sacred_texts_path = self.workspace_path / "backend" / "data_processing" / "data" / "sacred_texts_data.json"
            
            if not sacred_texts_path.exists():
                self.log_test_result("Sacred Texts Data", False, "Sacred texts data file not found")
                return False
                
            # Load and validate sacred texts
            with open(sacred_texts_path) as f:
                sacred_texts_data = json.load(f)
                
            # Extract texts array from the JSON structure
            if isinstance(sacred_texts_data, dict) and "texts" in sacred_texts_data:
                sacred_texts = sacred_texts_data["texts"]
            else:
                sacred_texts = sacred_texts_data
                
            if not sacred_texts or len(sacred_texts) == 0:
                self.log_test_result("Sacred Texts Content", False, "No sacred texts found")
                return False
                
            # Check structure - map 'text' to 'content' and add missing fields
            required_fields = ["id", "source", "chapter", "verse"]
            optional_fields = ["title", "content", "text"]  # Allow either content or text
            
            for i, text in enumerate(sacred_texts):
                if i >= 5:  # Only check first 5 texts
                    break
                missing_fields = [field for field in required_fields if field not in text]
                
                # Check if either 'content' or 'text' field exists
                has_content = "content" in text or "text" in text
                if not has_content:
                    missing_fields.append("content/text")
                    
                if missing_fields:
                    self.log_test_result("Sacred Text Structure", False, f"Missing fields: {missing_fields}")
                    return False
                    
            # Check vector database
            vector_db_path = self.workspace_path / "data" / "vector_storage"
            if vector_db_path.exists() and any(vector_db_path.iterdir()):
                self.log_test_result("Vector Database", True, f"Found {len(sacred_texts)} sacred texts")
            else:
                self.log_test_result("Vector Database", False, "Vector database not populated")
                return False
                
            return True
            
        except Exception as e:
            self.log_test_result("Sacred Text Database", False, f"Error: {e}")
            return False

    async def test_performance_load(self) -> bool:
        """Test system performance and load handling."""
        logger.info("‚ö° Testing Performance & Load...")
        
        try:
            # Single request performance
            start_time = time.time()
            
            response = requests.post(
                f"{self.base_url}/api/spiritual_guidance",
                json={
                    "query": "What is the meaning of life according to Vedanta?",
                    "language": "English"
                },
                timeout=30
            )
            
            response_time = time.time() - start_time
            
            if response.status_code != 200:
                self.log_test_result("Performance Test", False, f"HTTP {response.status_code}")
                return False
                
            # Performance thresholds
            if response_time > 30:
                self.log_test_result("Response Time", False, f"Too slow: {response_time:.2f}s")
                return False
            elif response_time > 15:
                self.log_test_result("Response Time", True, f"Acceptable: {response_time:.2f}s (consider optimization)")
            else:
                self.log_test_result("Response Time", True, f"Good: {response_time:.2f}s")
                
            # Concurrent requests test (light load)
            import concurrent.futures
            
            def make_request():
                return requests.post(
                    f"{self.base_url}/api/spiritual_guidance",
                    json={"query": "Quick test", "language": "English"},
                    timeout=20
                ).status_code
                
            concurrent_start = time.time()
            with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
                futures = [executor.submit(make_request) for _ in range(3)]
                results = [future.result() for future in concurrent.futures.as_completed(futures)]
                
            concurrent_time = time.time() - concurrent_start
            
            success_count = sum(1 for result in results if result == 200)
            
            if success_count >= 2:  # At least 2 out of 3 should succeed
                self.log_test_result("Concurrent Load", True, f"{success_count}/3 requests succeeded in {concurrent_time:.2f}s")
            else:
                self.log_test_result("Concurrent Load", False, f"Only {success_count}/3 requests succeeded")
                return False
                
            return True
            
        except Exception as e:
            self.log_test_result("Performance Load", False, f"Error: {e}")
            return False

    async def test_error_handling(self) -> bool:
        """Test error handling and fallback mechanisms."""
        logger.info("üõ°Ô∏è  Testing Error Handling...")
        
        try:
            # Test invalid request
            invalid_response = requests.post(
                f"{self.base_url}/api/spiritual_guidance",
                json={"invalid": "request"},
                timeout=10
            )
            
            # Should handle gracefully (400 or return fallback)
            if invalid_response.status_code in [400, 422]:
                self.log_test_result("Invalid Request Handling", True, "Properly rejected invalid request")
            elif invalid_response.status_code == 200:
                # Check if it returned fallback response
                data = invalid_response.json()
                if "response" in data:
                    self.log_test_result("Invalid Request Handling", True, "Returned fallback guidance")
                else:
                    self.log_test_result("Invalid Request Handling", False, "Invalid request succeeded unexpectedly")
                    return False
            else:
                self.log_test_result("Invalid Request Handling", False, f"Unexpected status: {invalid_response.status_code}")
                return False
                
            # Test empty question
            empty_response = requests.post(
                f"{self.base_url}/api/spiritual_guidance",
                json={"query": "", "language": "English"},
                timeout=10
            )
            
            if empty_response.status_code in [200, 400]:
                self.log_test_result("Empty Question Handling", True, "Handled empty question appropriately")
            else:
                self.log_test_result("Empty Question Handling", False, f"Status: {empty_response.status_code}")
                return False
                
            return True
            
        except Exception as e:
            self.log_test_result("Error Handling", False, f"Error: {e}")
            return False

    def generate_report(self):
        """Generate comprehensive test report."""
        print("\n" + "="*60)
        print("üïâÔ∏è  VIMARSH PRODUCTION VALIDATION REPORT")
        print("="*60)
        
        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results if result["passed"])
        failed_tests = total_tests - passed_tests
        
        print(f"\nüìä SUMMARY:")
        print(f"   Total Tests: {total_tests}")
        print(f"   ‚úÖ Passed: {passed_tests}")
        print(f"   ‚ùå Failed: {failed_tests}")
        print(f"   üìà Success Rate: {(passed_tests/total_tests*100):.1f}%")
        
        if failed_tests > 0:
            print(f"\n‚ùå FAILED TESTS:")
            for result in self.test_results:
                if not result["passed"]:
                    print(f"   ‚Ä¢ {result['test']}: {result['details']}")
                    
        print(f"\n‚úÖ SUCCESSFUL TESTS:")
        for result in self.test_results:
            if result["passed"]:
                print(f"   ‚Ä¢ {result['test']}: {result['details']}")
                
        # Production readiness assessment
        critical_tests = [
            "Environment Config", "Gemini Integration", "RAG Pipeline", 
            "Sacred Text Database", "Performance Test"
        ]
        
        critical_passed = sum(1 for result in self.test_results 
                            if result["test"] in critical_tests and result["passed"])
        
        print(f"\nüéØ PRODUCTION READINESS:")
        if critical_passed == len(critical_tests):
            print("   üöÄ READY FOR PRODUCTION!")
            print("   All critical systems are functioning correctly.")
        elif critical_passed >= len(critical_tests) * 0.8:
            print("   ‚ö†Ô∏è  MOSTLY READY - Minor issues to resolve")
            print("   Most critical systems working, address failed tests.")
        else:
            print("   ‚ùå NOT READY FOR PRODUCTION")
            print("   Critical systems need attention before deployment.")
            
        print(f"\nüìù NEXT STEPS:")
        if failed_tests == 0:
            print("   1. Deploy to production using scripts/deploy-production.sh")
            print("   2. Monitor system performance and logs")
            print("   3. Set up production monitoring and alerts")
        else:
            print("   1. Fix failed tests listed above")
            print("   2. Re-run validation: python scripts/production_validator.py")
            print("   3. Review configuration files and API keys")
            
        print(f"\nüïâÔ∏è  May your spiritual guidance system serve seekers well!")
        print("="*60)

    async def run_validation(self):
        """Run complete production validation suite."""
        try:
            self.print_banner()
            
            # Run all validation tests
            await self.test_environment_config()
            await self.test_gemini_integration()
            await self.test_rag_pipeline()
            await self.test_authentication_system()
            await self.test_sacred_text_database()
            await self.test_performance_load()
            await self.test_error_handling()
            
            # Generate final report
            self.generate_report()
            
            # Return overall success
            return all(result["passed"] for result in self.test_results)
            
        except KeyboardInterrupt:
            print("\n\n‚ùå Validation cancelled by user.")
            return False
        except Exception as e:
            print(f"\n\n‚ùå Validation failed: {e}")
            logger.exception("Validation error")
            return False

async def main():
    """Main entry point."""
    validator = ProductionValidator()
    success = await validator.run_validation()
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    asyncio.run(main())
